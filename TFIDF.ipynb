{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Before You Run\n",
        "make a `data` drectory and upload data (eval, test and train csvs)"
      ],
      "metadata": {
        "id": "IDh-HjxqmTcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir data"
      ],
      "metadata": {
        "id": "PuPsSNI5lZ1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install hazm\n",
        "! pip install hazm"
      ],
      "metadata": {
        "id": "qPEKiawli0OZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cf3e5f4-7783-4c25-e60f-746f1e549822"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hazm in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: libwapiti>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from hazm) (0.2.1)\n",
            "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.7/dist-packages (from hazm) (3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "I0YkX5CXepfi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "07ab578b-d556-407c-a357-77c15b17be3b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from hazm import word_tokenize, Normalizer, Lemmatizer, Stemmer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "\n",
        "from sklearn.model_selection import PredefinedSplit, RandomizedSearchCV\n",
        "from sklearn.pipeline import Pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "WHmB8VnZIqBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "NIvkU8e2eSm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = 'data/'\n",
        "PATH = PATH.rstrip('/')\n",
        "\n",
        "# Train\n",
        "df_train = pd.read_csv(PATH + '/train.csv')\n",
        "df_train.columns = ['index', 'comment', 'rate']\n",
        "\n",
        "# Evaluation\n",
        "df_eval = pd.read_csv(PATH + '/eval.csv')\n",
        "df_eval.columns = ['index', 'comment', 'rate']\n",
        "\n",
        "# Test\n",
        "df_test = pd.read_csv(PATH + '/test.csv')\n",
        "df_test.columns = ['index', 'comment', 'rate']\n",
        "\n",
        "# Create Lables\n",
        "label_encoder = LabelEncoder()\n",
        "# Y\n",
        "train_y = label_encoder.fit_transform((df_train['rate'] >= 0).astype(int))\n",
        "eval_y = label_encoder.fit_transform((df_eval['rate'] >= 0).astype(int))\n",
        "test_y = label_encoder.fit_transform((df_test['rate'] >= 0).astype(int))"
      ],
      "metadata": {
        "id": "yBYACIpBxCaa"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess"
      ],
      "metadata": {
        "id": "g3UJaMTV6B9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalizer() # Hazm normlizer\n",
        "lemmatize = Lemmatizer().lemmatize # Hazm lemmatizer\n",
        "stem = Stemmer().stem # Hazm Stemmer\n",
        "clean_lemmatize = lambda comment:[lemmatize(word) if '#' not in lemmatize(word) else word for word in comment]\n",
        "clean_stem = lambda comment:[stem(word) for word in comment]\n",
        "\n",
        "symbols_complete_reg = re.compile(r\"(\\d|\\\"|'ٍ|¬|[؛“،,”‘۔’’‘–]|[|\\.÷+\\]\\[\\)\\(\\:\\-\\?»\\=\\{}\\*«»_…\\؟!/ـ]|[۰'ٓ۫'ٔ]|[ٓٔ]|[ًٌٍْﹼ،َُِّ«ٰ»ٖء])\")\n",
        "\n",
        "def remeove_arabic(text):\n",
        "    # remove arabic alphabet\n",
        "    mapping = {\n",
        "        u\"ۀ\" : u\"ه\",\n",
        "        u\"ة\" : u\"ت\",\n",
        "        u\"ي\" : u\"ی\",\n",
        "        u\"ؤ\" : u\"و\",\n",
        "        u\"إ\" : u\"ا\",\n",
        "        u\"ٹ\" : u\"ت\",\n",
        "        u\"ڈ\" : u\"د\",\n",
        "        u\"ئ\" : u\"ی\",\n",
        "        u\"ﻨ\" : u\"ن\",\n",
        "        u\"ﺠ\" : u\"ج\",\n",
        "        u\"ﻣ\" : u\"م\",\n",
        "        u\"ﷲ\" : u\"\",\n",
        "        u\"ﻳ\" : u\"ی\",\n",
        "        u\"ٻ\" : u\"ب\",\n",
        "        u\"ٱ\" : u\"ا\",\n",
        "        u\"ڵ\" : u\"ل\",\n",
        "        u\"ﭘ\" : u\"پ\",\n",
        "        u\"ﻪ\" : u\"ه\",\n",
        "        u\"ﻳ\" : u\"ی\",\n",
        "        u\"ٻ\" : u\"ب\",\n",
        "        u\"ں\" : u\"ن\",\n",
        "        u\"ٶ\" : u\"و\",\n",
        "        u\"ٲ\" : u\"ا\",\n",
        "        u\"ہ\" : u\"ه\",\n",
        "        u\"ﻩ\" : u\"ه\",\n",
        "        u\"ﻩ\" : u\"ه\",\n",
        "        u\"ك\" : u\"ک\",\n",
        "        u\"ﺆ\" : u\"و\",\n",
        "        u\"أ\" : u\"ا\",\n",
        "        u\"ﺪ\" : u\"د\"\n",
        "    }\n",
        "    arabic_keys =  re.compile(r\"(\" + \"|\".join(mapping.keys()) + r\")\")\n",
        "    return arabic_keys.sub(lambda x: mapping[x.group()], text)\n",
        "\n",
        "\n",
        "# clean_text function\n",
        "def clean_comment(text, allspace=True, punc=True, sentence=True, only_persian=True):\n",
        "    #remove halph space, new line ('\\n') and '\\r'\n",
        "    text = text.replace('\\u200c', ' ').replace('\\n', '').replace('\\r', '')\n",
        "    # remove punctuations\n",
        "    text = re.sub(symbols_complete_reg, \"\", text)\n",
        "    # remove arabic letters\n",
        "    text = remeove_arabic(text)\n",
        "    # convert spaces to a one space and delete leading and trailing spaces\n",
        "    text = re.sub(\"(\\s)+\", \" \", text)\n",
        "    text = text.strip()\n",
        "    #lemmatize\n",
        "    \" \".join(clean_lemmatize(text.split(\" \")))\n",
        "    #stemming\n",
        "    \" \".join(clean_stem(text.split(\" \")))\n",
        "    # convert spaces to a one space and delete leading and trailing spaces\n",
        "    text = re.sub(\"(\\s)+\", \" \", text)\n",
        "    text = text.strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "IGXA192_6FX0"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X : Features\n",
        "df_train['clean_comment'] = df_train['comment'].apply(lambda comment:clean_comment(comment))\n",
        "df_eval['clean_comment'] = df_eval['comment'].apply(lambda comment:clean_comment(comment))\n",
        "df_test['clean_comment'] = df_test['comment'].apply(lambda comment:clean_comment(comment))"
      ],
      "metadata": {
        "id": "qFrcOt1w9i8n"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_id = 500\n",
        "example = df_train['clean_comment'][example_id]\n",
        "example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OtPg_moO-T3Y",
        "outputId": "fd98ecca-b3fc-42f3-f0bc-be78d738ad22"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'خیلی عالیه'"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TFIDF and Logistic Regression"
      ],
      "metadata": {
        "id": "bc916OYYIzEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Base Line Accuracy"
      ],
      "metadata": {
        "id": "zOneB4LGmbRs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0WNLoAQhHZs",
        "outputId": "5885a402-07a4-4000-cf8e-5af27e622c1e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6941176470588235"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "# accuracy base line\n",
        "np.sum(test_y)/len(test_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline \n",
        "This pipeline is designed for finding the best parameters in both tfidf and lr model. Finding best parameter is based on accuracy of both models in the validation data.\n",
        "To aim this we use `RandomizedSearchCV` method. \n"
      ],
      "metadata": {
        "id": "7mERgGme1EWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# solvers= ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
        "# multi_classes = ['multinomial', 'ovr']\n",
        "\n",
        "pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(analyzer='word')),\n",
        "        ('lr', LogisticRegression(random_state=0, solver='liblinear', max_iter=1000, multi_class='ovr'))\n",
        "    ])"
      ],
      "metadata": {
        "id": "SRIyKJ1Z4Yo1"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {\n",
        "    'lr__C': (0.01, 0.1, 2, 5, 10, 15, 20),\n",
        "    'lr__penalty': ('l1', 'l2'),\n",
        "    'tfidf__min_df': (0, 1, 3, 5),\n",
        "    'tfidf__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
        "    'tfidf__max_features': (None, 2000, 8000, 12000, 15000)\n",
        "}"
      ],
      "metadata": {
        "id": "ja2h7OVa5NI9"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_index = [-1] *len(df_train['clean_comment']) + [0] *len(df_eval['clean_comment'])\n",
        "X_train_eval_joint = list(df_train['clean_comment']) + list(df_eval['clean_comment'])\n",
        "Y_train_eval_joint = list(train_y) + list(eval_y)"
      ],
      "metadata": {
        "id": "3f0jeBOM8Q9C"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PredefinedSplit(test_fold=split_index)"
      ],
      "metadata": {
        "id": "8idpBpJ97NnN"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid = RandomizedSearchCV(pipeline, parameters, scoring='accuracy', cv=ps)"
      ],
      "metadata": {
        "id": "sC77BELQ6zSw"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid.fit(X_train_eval_joint, Y_train_eval_joint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Zvdog1K6Vw-",
        "outputId": "c2ad4c71-a1f9-43c1-ee97-8bf662143250"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
              "                   estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
              "                                             ('lr',\n",
              "                                              LogisticRegression(max_iter=1000,\n",
              "                                                                 multi_class='ovr',\n",
              "                                                                 random_state=0,\n",
              "                                                                 solver='liblinear'))]),\n",
              "                   param_distributions={'lr__C': (0.01, 0.1, 2, 5, 10, 15, 20),\n",
              "                                        'lr__penalty': ('l1', 'l2'),\n",
              "                                        'tfidf__max_features': (None, 2000,\n",
              "                                                                8000, 12000,\n",
              "                                                                15000),\n",
              "                                        'tfidf__min_df': (0, 1, 3, 5),\n",
              "                                        'tfidf__ngram_range': ((1, 1), (1, 2),\n",
              "                                                               (1, 3))},\n",
              "                   scoring='accuracy')"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best accuracy in the pipline: {grid.best_score_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lM9pP_j6ti1",
        "outputId": "be1f9326-23eb-4536-f953-2a863baf82a8"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best accuracy in the pipline: 0.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" best f1: \n",
        "{'lr__C': 0.1,\n",
        " 'lr__penalty': 'l1',\n",
        " 'tfidf__max_features': 12000,\n",
        " 'tfidf__min_df': 3,\n",
        " 'tfidf__ngram_range': (1, 1)}\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "best accuracy:\n",
        "{'lr__C': 15,\n",
        " 'lr__penalty': 'l2',\n",
        " 'tfidf__max_features': 12000,\n",
        " 'tfidf__min_df': 1,\n",
        " 'tfidf__ngram_range': (1, 1)}\n",
        "\"\"\"\n",
        "\n",
        "print(\"Best parameters in pipeline (accuracy):\")\n",
        "grid.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXWNnsm66uI0",
        "outputId": "1a2dd49a-53a2-44b3-ca3b-ec959f74b925"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters in pipeline (accuracy):\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lr__C': 15,\n",
              " 'lr__penalty': 'l2',\n",
              " 'tfidf__max_features': 12000,\n",
              " 'tfidf__min_df': 1,\n",
              " 'tfidf__ngram_range': (1, 1)}"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TFIDF"
      ],
      "metadata": {
        "id": "neS9NoDYJ0Xx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKFB3cv_hHZv",
        "outputId": "f112f422-8386-4372-aa25-0ce98b17aa25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(800, 4273)\n"
          ]
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer(min_df=1, ngram_range = (1,1), max_features=12000)\n",
        "train_data_features = vectorizer.fit_transform(df_train['clean_comment'])\n",
        "print(train_data_features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "MafEgoO5hHZw"
      },
      "outputs": [],
      "source": [
        "## data snooping ALERT: we should transforom not fit again\n",
        "eval_data_features = vectorizer.transform(df_eval['clean_comment'])\n",
        "test_data_features = vectorizer.transform(df_test['clean_comment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oBu6m35hHZw",
        "outputId": "b0e3ca19-bf30-43fd-e153-8a778fa67f09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['آدمو',\n",
              " 'آذرماه',\n",
              " 'آذرگانم',\n",
              " 'آرام',\n",
              " 'آردن',\n",
              " 'آرزو',\n",
              " 'آروستو',\n",
              " 'آری',\n",
              " 'آز',\n",
              " 'آزاد']"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ],
      "source": [
        "# show\n",
        "vectorizer.get_feature_names()[200:210]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "AXQ1LUtZJ4Gp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4IR9ytHhHZx",
        "outputId": "84ac8377-19c5-43b8-b585-fac687be0ba9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=15, max_iter=1000, multi_class='ovr', random_state=0,\n",
              "                   solver='liblinear')"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ],
      "source": [
        "# Load model\n",
        "\n",
        "model = LogisticRegression(C=15, penalty='l2', random_state=0, solver='liblinear', max_iter=1000, multi_class='ovr')\n",
        "# Train model\n",
        "model.fit(train_data_features, train_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "6l_c_Iy_KF-x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "7ap8p4ThhHZy"
      },
      "outputs": [],
      "source": [
        "## evaluation on test data\n",
        "y_test_pred = model.predict(test_data_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRFA7wZshHZy",
        "outputId": "cdf986df-4d84-4434-8772-035d5d241b6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Accuracy Score ----- \n",
            "0.7352941176470589\n",
            "----- Confusion Matrix ----- \n",
            "[[ 18  34]\n",
            " [ 11 107]]\n",
            "----- Classification Report ----- \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.35      0.44        52\n",
            "           1       0.76      0.91      0.83       118\n",
            "\n",
            "    accuracy                           0.74       170\n",
            "   macro avg       0.69      0.63      0.64       170\n",
            "weighted avg       0.72      0.74      0.71       170\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# On test data\n",
        "print('----- Accuracy Score ----- ')\n",
        "print(accuracy_score(test_y, y_test_pred))\n",
        "print('----- Confusion Matrix ----- ')\n",
        "print(confusion_matrix(test_y, y_test_pred))\n",
        "print('----- Classification Report ----- ')\n",
        "print(classification_report(test_y, y_test_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Features"
      ],
      "metadata": {
        "id": "06frHdfNKJ4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def top_key_features(vectorizer, model, n_top=30):\n",
        "    weights = model.coef_\n",
        "    feature_names = vectorizer.get_feature_names()\n",
        "    sorted_features = weights[0].argsort()[::-1]\n",
        "    most_important = sorted_features[:n_top]\n",
        "    least_important = sorted_features[-n_top:]\n",
        "\n",
        "    print('Most important words in the class 1: \\n')\n",
        "    for i in most_important:\n",
        "        print(f\"{feature_names[i]}: {weights[0, i]}\")\n",
        "\n",
        "    print('Most important words in the class 2: \\n')\n",
        "    for i in least_important:\n",
        "        print(f\"{feature_names[i]}: {weights[0, i]}\")\n"
      ],
      "metadata": {
        "id": "Zcsr1csYDvdZ"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Most Important Features (Top 30)"
      ],
      "metadata": {
        "id": "t0ymSIj1KQOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_key_features(vectorizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DITLs-2rDwEe",
        "outputId": "00315deb-251b-461a-e911-2405665cf037"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most important words in the class 1: \n",
            "\n",
            "دوربین: 3.23638261564184\n",
            "نیز: 3.2268381428187576\n",
            "digikala: 2.5309529271587725\n",
            "تو: 2.479308604417539\n",
            "اینکه: 2.4694945869725706\n",
            "واقعا: 2.4462823368506523\n",
            "وجود: 2.3987426101027483\n",
            "همین: 2.3792914670190344\n",
            "می: 2.326414357430155\n",
            "هست: 2.282688498825936\n",
            "نسبت: 2.259165864957199\n",
            "قشنگ: 2.233477102291063\n",
            "دستگاه: 2.218611980912459\n",
            "کفش: 2.1748800026603132\n",
            "رینگ: 2.0940869189893143\n",
            "نصب: 2.092673687316176\n",
            "نبود: 2.086794772131762\n",
            "شده: 2.0393511467273053\n",
            "پیش: 2.0336723271515424\n",
            "نظر: 2.013301657967153\n",
            "تقریبا: 2.0045345996962887\n",
            "طعمش: 1.976633655219301\n",
            "گوشفیل: 1.9611449450543894\n",
            "کاملا: 1.9409312724655396\n",
            "شارژ: 1.9182286786984186\n",
            "موجود: 1.8853972547772857\n",
            "کنه: 1.8686890072866533\n",
            "جعبه: 1.8662110840563089\n",
            "مونده: 1.862958366737985\n",
            "کار: 1.8509578934361548\n",
            "Most important words in the class 2: \n",
            "\n",
            "ممنون: -2.585981917605968\n",
            "پیشنهاد: -2.601150890635267\n",
            "اصن: -2.6590737895816665\n",
            "ارزه: -2.6865214236215222\n",
            "میتونست: -2.700873505236188\n",
            "زود: -2.7349909580529834\n",
            "قیمتی: -2.736748778734155\n",
            "داشتم: -2.742322547359291\n",
            "ایزو: -2.74815022072744\n",
            "گران: -2.783841857805191\n",
            "نبودم: -2.807553418767238\n",
            "خارجی: -2.861727566263604\n",
            "میدان: -2.8896879792089\n",
            "جنساش: -3.0736316644023147\n",
            "هزینه: -3.12127795430147\n",
            "چرت: -3.1470532380685987\n",
            "نخرید: -3.1710191621410657\n",
            "خوشمزه: -3.1895338170461547\n",
            "خوش: -3.240596869286545\n",
            "موقع: -3.256814634687412\n",
            "اصلا: -3.3689189682318483\n",
            "سریع: -3.371894314393841\n",
            "همیشه: -3.3760117987597438\n",
            "لک: -3.3776581342706042\n",
            "معمولی: -3.4630835296163816\n",
            "چندان: -3.5087714683679874\n",
            "کیفیت: -3.594619493143974\n",
            "ارسال: -3.6211621559710023\n",
            "دقیقه: -3.9098751181828355\n",
            "سنگینه: -4.043723145905925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "TFIDF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}