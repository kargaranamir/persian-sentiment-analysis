{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Before You Run\n",
        "make a `data` drectory and upload data (eval, test and train csvs)"
      ],
      "metadata": {
        "id": "IDh-HjxqmTcj"
      },
      "id": "IDh-HjxqmTcj"
    },
    {
      "cell_type": "code",
      "source": [
        "# ! mkdir data"
      ],
      "metadata": {
        "id": "PuPsSNI5lZ1Z"
      },
      "id": "PuPsSNI5lZ1Z",
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install fasttext\n",
        "! pip install fasttext\n",
        "\n",
        "# install gdown to download fasttext model from google drive (with maximum speed!)\n",
        "! pip install gdown\n",
        "\n",
        "# install matplotlib to prevent unwelcome errors\n",
        "! pip install matplotlib==3.1.3\n",
        "\n",
        "! pip install hazm"
      ],
      "metadata": {
        "id": "DTKXZusSQNl1"
      },
      "id": "DTKXZusSQNl1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "I0YkX5CXepfi"
      },
      "id": "I0YkX5CXepfi"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "07ab578b-d556-407c-a357-77c15b17be3b",
      "metadata": {
        "id": "07ab578b-d556-407c-a357-77c15b17be3b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf \n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout, SpatialDropout1D\n",
        "from tensorflow.keras.layers import GlobalMaxPool1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import fasttext\n",
        "\n",
        "from hazm import word_tokenize, Normalizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "NIvkU8e2eSm3"
      },
      "id": "NIvkU8e2eSm3"
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = 'data/'\n",
        "PATH = PATH.rstrip('/')\n",
        "\n",
        "# Train\n",
        "df_train = pd.read_csv(PATH + '/train.csv')\n",
        "df_train.columns = ['index', 'comment', 'rate']\n",
        "\n",
        "# Evaluation\n",
        "df_eval = pd.read_csv(PATH + '/eval.csv')\n",
        "df_eval.columns = ['index', 'comment', 'rate']\n",
        "\n",
        "# Test\n",
        "df_test = pd.read_csv(PATH + '/test.csv')\n",
        "df_test.columns = ['index', 'comment', 'rate']\n",
        "\n",
        "# Create Lables\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "train_y = label_encoder.fit_transform((df_train['rate'] >= 0).astype(int))\n",
        "eval_y = label_encoder.fit_transform((df_eval['rate'] >= 0).astype(int))\n",
        "test_y = label_encoder.fit_transform((df_test['rate'] >= 0).astype(int))"
      ],
      "metadata": {
        "id": "yBYACIpBxCaa"
      },
      "id": "yBYACIpBxCaa",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess"
      ],
      "metadata": {
        "id": "g3UJaMTV6B9F"
      },
      "id": "g3UJaMTV6B9F"
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalizer() # Hazm normlizer\n",
        "symbols_complete_reg = re.compile(r\"(\\d|\\\"|'ٍ|¬|[؛“،,”‘۔’’‘–]|[|\\.÷+\\]\\[\\)\\(\\:\\-\\?»\\=\\{}\\*«»_…\\؟!/ـ]|[۰'ٓ۫'ٔ]|[ٓٔ]|[ًٌٍْﹼ،َُِّ«ٰ»ٖء])\")\n",
        "\n",
        "def remeove_arabic(text):\n",
        "    # remove arabic alphabet\n",
        "    mapping = {\n",
        "        u\"ۀ\" : u\"ه\",\n",
        "        u\"ة\" : u\"ت\",\n",
        "        u\"ي\" : u\"ی\",\n",
        "        u\"ؤ\" : u\"و\",\n",
        "        u\"إ\" : u\"ا\",\n",
        "        u\"ٹ\" : u\"ت\",\n",
        "        u\"ڈ\" : u\"د\",\n",
        "        u\"ئ\" : u\"ی\",\n",
        "        u\"ﻨ\" : u\"ن\",\n",
        "        u\"ﺠ\" : u\"ج\",\n",
        "        u\"ﻣ\" : u\"م\",\n",
        "        u\"ﷲ\" : u\"\",\n",
        "        u\"ﻳ\" : u\"ی\",\n",
        "        u\"ٻ\" : u\"ب\",\n",
        "        u\"ٱ\" : u\"ا\",\n",
        "        u\"ڵ\" : u\"ل\",\n",
        "        u\"ﭘ\" : u\"پ\",\n",
        "        u\"ﻪ\" : u\"ه\",\n",
        "        u\"ﻳ\" : u\"ی\",\n",
        "        u\"ٻ\" : u\"ب\",\n",
        "        u\"ں\" : u\"ن\",\n",
        "        u\"ٶ\" : u\"و\",\n",
        "        u\"ٲ\" : u\"ا\",\n",
        "        u\"ہ\" : u\"ه\",\n",
        "        u\"ﻩ\" : u\"ه\",\n",
        "        u\"ﻩ\" : u\"ه\",\n",
        "        u\"ك\" : u\"ک\",\n",
        "        u\"ﺆ\" : u\"و\",\n",
        "        u\"أ\" : u\"ا\",\n",
        "        u\"ﺪ\" : u\"د\"\n",
        "    }\n",
        "    arabic_keys =  re.compile(r\"(\" + \"|\".join(mapping.keys()) + r\")\")\n",
        "    return arabic_keys.sub(lambda x: mapping[x.group()], text)\n",
        "\n",
        "\n",
        "# clean_text function\n",
        "def clean_comment(text, allspace=True, punc=True, sentence=True, only_persian=True):\n",
        "    #remove halph space, new line ('\\n') and '\\r'\n",
        "    text = text.replace('\\u200c', ' ').replace('\\n', '').replace('\\r', '')\n",
        "    # remove punctuations\n",
        "    text = re.sub(symbols_complete_reg, \"\", text)\n",
        "    # remove arabic letters\n",
        "    text = remeove_arabic(text)\n",
        "    # convert spaces to a one space and delete leading and trailing spaces\n",
        "    text = re.sub(\"(\\s)+\", \" \", text)\n",
        "    text = text.strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "IGXA192_6FX0"
      },
      "id": "IGXA192_6FX0",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['clean_comment'] = df_train['comment'].apply(lambda comment:clean_comment(comment))\n",
        "df_eval['clean_comment'] = df_eval['comment'].apply(lambda comment:clean_comment(comment))\n",
        "df_test['clean_comment'] = df_test['comment'].apply(lambda comment:clean_comment(comment))"
      ],
      "metadata": {
        "id": "qFrcOt1w9i8n"
      },
      "id": "qFrcOt1w9i8n",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_id = 500\n",
        "example = df_train['clean_comment'][example_id]\n",
        "example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OtPg_moO-T3Y",
        "outputId": "e80352db-e47f-4eb8-a9cc-bbe23aa20d39"
      },
      "id": "OtPg_moO-T3Y",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'خیلی عالیه'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1796d3e-6920-44fd-96a0-892bb67ce303",
      "metadata": {
        "id": "d1796d3e-6920-44fd-96a0-892bb67ce303"
      },
      "source": [
        "## FastText Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Skipgram Model"
      ],
      "metadata": {
        "id": "S5mrfStCR2zA"
      },
      "id": "S5mrfStCR2zA"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8d9e7770-c460-44ec-b8b4-213befad7c44",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d9e7770-c460-44ec-b8b4-213befad7c44",
        "outputId": "13336ea9-e9d9-4bc4-f388-b980bd19f61c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-10 13:14:16--  https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.fa.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3631863356 (3.4G) [application/zip]\n",
            "Saving to: ‘wiki.fa.zip’\n",
            "\n",
            "wiki.fa.zip         100%[===================>]   3.38G  34.3MB/s    in 1m 41s  \n",
            "\n",
            "2022-01-10 13:15:57 (34.3 MB/s) - ‘wiki.fa.zip’ saved [3631863356/3631863356]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Model 1: Dimension: 100 from # https://github.com/taesiri/PersianWordVectors\n",
        "# SKIPGRAM_MODEL_FILE_ID_1 = '1wPnMG9_GNUVdSgbznQziQc5nMWI3QKNz'\n",
        "# !gdown --id $SKIPGRAM_MODEL_FILE_ID \n",
        "\n",
        "# Model 2: Dimension: 300 from https://fasttext.cc/docs/en/pretrained-vectors.html\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.fa.zip\n",
        "! unzip wiki.fa.zip\n",
        "! rm -rf wiki.fa.zip\n",
        "! rm -rf wiki.fa.vec\n",
        "EMBEDDING_LEN = 300 # 100 for Model 1 and 300 for Model 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load FastText Model"
      ],
      "metadata": {
        "id": "GXOumsIWR9YX"
      },
      "id": "GXOumsIWR9YX"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "7d0d02a1-504f-4da8-9254-f31b7386cd32",
      "metadata": {
        "id": "7d0d02a1-504f-4da8-9254-f31b7386cd32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b98690b-b3b8-49c9-9e1d-2636296133e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "# Model 1:\n",
        "# model_skipgram = fasttext.load_model('farsi-dedup-skipgram.bin')\n",
        "# Model 2:\n",
        "model_skipgram = fasttext.load_model('wiki.fa.bin')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit Keras Tokenizer on comments\n",
        "comments = df_train['clean_comment'].values\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=3000)\n",
        "tokenizer.fit_on_texts(comments)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size : {}'.format(vocab_size))"
      ],
      "metadata": {
        "id": "_WQEhOzDS8Yw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c27b7752-ac92-43bd-da31-b93da715a8a4"
      },
      "id": "_WQEhOzDS8Yw",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size : 4323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_comments = tokenizer.texts_to_sequences(comments)\n",
        "\n",
        "# example of encoded comments\n",
        "print(\"Comment : {}\".format(comments[1]))\n",
        "print(\"Corresponding Encoding : {}\".format(encoded_comments[1]))"
      ],
      "metadata": {
        "id": "Bs7z2ltRTNCA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6f441c9-1b91-46f8-fb9d-f426ebb3f061"
      },
      "id": "Bs7z2ltRTNCA",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comment : سلام به دوستای عزیزم عزاداری هاتون قبول باشه\n",
            "Corresponding Encoding : [94, 2, 1716, 817, 1717, 818, 526, 68]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "48de0848-6da0-4c32-b156-d2bdf08a959d",
      "metadata": {
        "id": "48de0848-6da0-4c32-b156-d2bdf08a959d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57c59f11-11c1-4def-e70c-ef9c97d5b6a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padding Shape: (800, 616)\n"
          ]
        }
      ],
      "source": [
        "# padding\n",
        "SENT_MAX_LEN = max([len(sent) for sent in encoded_comments])\n",
        "padded_sequence = pad_sequences(encoded_comments, maxlen=SENT_MAX_LEN, padding='post')\n",
        "print('Padding Shape: {}'.format(padded_sequence.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "114a59a6-c98a-4659-9eff-ff976fb4b8e6",
      "metadata": {
        "id": "114a59a6-c98a-4659-9eff-ff976fb4b8e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1344d0ba-891e-445c-b68b-9eaf4ff0cf5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding Matrix Shape is: (4323, 300)\n"
          ]
        }
      ],
      "source": [
        "# initial embedding matrix\n",
        "embedding_matrix = np.zeros((vocab_size, EMBEDDING_LEN))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  embedding_vector = model_skipgram.get_word_vector(word)\n",
        "  # words that cannot be found will be set to 0\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "\n",
        "print(f\"Embedding Matrix Shape is: {embedding_matrix.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "81b4d562-56d1-4a94-81d3-7d5899399d50",
      "metadata": {
        "id": "81b4d562-56d1-4a94-81d3-7d5899399d50"
      },
      "outputs": [],
      "source": [
        "# Same procedure with a Unique Tokenizer on Evaluation data\n",
        "eval_comments = df_eval['clean_comment'].values\n",
        "tokenizer.texts_to_matrix(eval_comments)\n",
        "eval_encoded_comments = tokenizer.texts_to_sequences(eval_comments)\n",
        "eval_padded_sequence = pad_sequences(eval_encoded_comments, maxlen=SENT_MAX_LEN, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "ae83164d-2b20-4b8b-be62-8317a0c3b29a",
      "metadata": {
        "id": "ae83164d-2b20-4b8b-be62-8317a0c3b29a"
      },
      "outputs": [],
      "source": [
        "# Same procedure with a Unique Tokenizer on Test data\n",
        "test_comments = df_test['clean_comment'].values\n",
        "tokenizer.texts_to_matrix(test_comments)\n",
        "test_encoded_comments = tokenizer.texts_to_sequences(test_comments)\n",
        "test_padded_sequence = pad_sequences(test_encoded_comments, maxlen=SENT_MAX_LEN, padding='post')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM Model Architecture"
      ],
      "metadata": {
        "id": "adHsUq4GUScc"
      },
      "id": "adHsUq4GUScc"
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM constants\n",
        "LSTM_UNITS = 32"
      ],
      "metadata": {
        "id": "dYh33GkGioq-"
      },
      "id": "dYh33GkGioq-",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, EMBEDDING_LEN, input_length=SENT_MAX_LEN, weights=[embedding_matrix], trainable=True))\n",
        "model.add(Bidirectional(LSTM(EMBEDDING_LEN, return_sequences=True, input_shape=(None, 1))))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(LSTM(LSTM_UNITS)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(EMBEDDING_LEN, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tkv0Pr89RR_S",
        "outputId": "0605a127-e719-43a5-890d-7d61af534832"
      },
      "id": "Tkv0Pr89RR_S",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_10 (Embedding)    (None, 616, 300)          1296900   \n",
            "                                                                 \n",
            " bidirectional_7 (Bidirectio  (None, 616, 600)         1442400   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 616, 600)          0         \n",
            "                                                                 \n",
            " bidirectional_8 (Bidirectio  (None, 64)               162048    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 300)               19500     \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 300)               0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1)                 301       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,921,149\n",
            "Trainable params: 2,921,149\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fit LSTM Model"
      ],
      "metadata": {
        "id": "RJyJIjngVuUm"
      },
      "id": "RJyJIjngVuUm"
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "6ede4981-e708-4956-9736-d7b0b6a65f59",
      "metadata": {
        "id": "6ede4981-e708-4956-9736-d7b0b6a65f59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0472224-20ba-451e-a39f-3bc5191e5477"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "25/25 [==============================] - 29s 921ms/step - loss: 0.5780 - accuracy: 0.7475 - val_loss: 0.5876 - val_accuracy: 0.7250\n",
            "Epoch 2/5\n",
            "25/25 [==============================] - 21s 851ms/step - loss: 0.5626 - accuracy: 0.7575 - val_loss: 0.5795 - val_accuracy: 0.7250\n",
            "Epoch 3/5\n",
            "25/25 [==============================] - 21s 851ms/step - loss: 0.5335 - accuracy: 0.7575 - val_loss: 0.5534 - val_accuracy: 0.7250\n",
            "Epoch 4/5\n",
            "25/25 [==============================] - 21s 850ms/step - loss: 0.4955 - accuracy: 0.7700 - val_loss: 0.5536 - val_accuracy: 0.7300\n",
            "Epoch 5/5\n",
            "25/25 [==============================] - 21s 850ms/step - loss: 0.4105 - accuracy: 0.8025 - val_loss: 0.6190 - val_accuracy: 0.7400\n"
          ]
        }
      ],
      "source": [
        "model = model.fit(\n",
        "    padded_sequence, \n",
        "    train_y, \n",
        "    batch_size=32, \n",
        "    epochs=5, \n",
        "    validation_data=(eval_padded_sequence, eval_y)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_lstm, acc_lstm = model.model.evaluate(test_padded_sequence, test_y, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc_lstm*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAko7El4vMkr",
        "outputId": "6eb8d38b-13d9-4307-bc05-bf98e0812be0"
      },
      "id": "tAko7El4vMkr",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 75.882351\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 75.882351"
      ],
      "metadata": {
        "id": "GfsNqbzpXvIb"
      },
      "id": "GfsNqbzpXvIb",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}